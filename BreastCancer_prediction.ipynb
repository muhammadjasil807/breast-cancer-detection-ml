{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8PftgbayUb6"
      },
      "outputs": [],
      "source": [
        "# Step 1 ‚Äì Install Required Libraries\n",
        "# Purpose:\n",
        "# Install essential ML packages to handle data, train the model, and build a Streamlit app.\n",
        "# We'll use:\n",
        "# - scikit-learn for ML algorithms\n",
        "# - pandas for handling tabular data\n",
        "# - joblib for saving and loading the trained model\n",
        "# - streamlit for building an interactive app\n",
        "# - pyngrok for hosting the app publicly (optional)\n",
        "\n",
        "# Install required Python packages quietly (no verbose output).\n",
        "!pip install scikit-learn pandas joblib streamlit pyngrok --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 ‚Äì Import Libraries and Load Dataset\n",
        "# Purpose:\n",
        "# Import necessary Python libraries and load the built-in Breast Cancer dataset from scikit-learn.\n",
        "# We'll convert it into a pandas DataFrame and explore basic information.\n",
        "\n",
        "# Import pandas for DataFrame operations (tabular data handling)\n",
        "import pandas as pd\n",
        "\n",
        "# Import the built-in Breast Cancer dataset from scikit-learn\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the dataset into memory\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Create a DataFrame 'df' containing all input features (independent variables)\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "# Add a new column 'target' which contains output labels (0 = malignant, 1 = benign)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Print the dataset's shape (number of rows and columns)\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "\n",
        "# Display the first few rows of the dataset to get an overview\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "cNQEGIgrydZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3 ‚Äì Exploratory Data Analysis (EDA)\n",
        "# Purpose:\n",
        "# Understand the dataset by viewing statistical information, class distribution, and checking for missing values.\n",
        "\n",
        "# Show basic statistical summary (mean, std, min, max) for each column\n",
        "print(df.describe())\n",
        "\n",
        "# Check for missing values in each column (should be zero for clean datasets)\n",
        "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
        "\n",
        "# Check class balance (number of benign vs malignant cases)\n",
        "print(\"\\nClass Distribution (0 = Malignant, 1 = Benign):\")\n",
        "print(df['target'].value_counts())\n"
      ],
      "metadata": {
        "id": "8xFiuHrcymWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4 ‚Äì Select the same 5 features used in the Streamlit app, then split data for training/testing.\n",
        "# Purpose:\n",
        "# Keep the dataset consistent with app inputs to avoid \"missing feature\" errors later.\n",
        "\n",
        "# Define the feature names that match the Streamlit input fields.\n",
        "selected_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness']\n",
        "\n",
        "# Create the feature matrix X with only these 5 columns.\n",
        "X = df[selected_features]\n",
        "\n",
        "# Create the target vector y from the 'target' column.\n",
        "y = df['target']\n",
        "\n",
        "# Import the function to split data into train/test parts.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split X and y into training (80%) and testing (20%) sets.\n",
        "# random_state ensures reproducibility.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Confirm shapes of the split data.\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "TevPhQCqyyHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 ‚Äì Standardize numeric features.\n",
        "# Purpose:\n",
        "# Scale all features so they have mean 0 and standard deviation 1, which helps the model converge faster.\n",
        "\n",
        "# Import StandardScaler for normalization.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler.\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on training data and transform it.\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Apply the same transformation to test data.\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Print confirmation message.\n",
        "print(\"‚úÖ Feature scaling completed.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QOdM6oMGy3mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6 ‚Äì Train and Evaluate Model\n",
        "# Purpose:\n",
        "# Fit a Logistic Regression model to the scaled training data and evaluate its performance.\n",
        "\n",
        "# Import LogisticRegression from scikit-learn.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the model with higher iteration limit to ensure convergence.\n",
        "model = LogisticRegression(max_iter=500, random_state=42)\n",
        "\n",
        "# Train the model on the scaled training data.\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Use the model to predict labels for the test set.\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Import accuracy metric to check performance.\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Calculate accuracy.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print performance results.\n",
        "print(\"‚úÖ Model training completed successfully!\")\n",
        "print(\"Accuracy on test data:\", round(accuracy, 4))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "CMZ01DNAy6Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7 ‚Äì Save the trained model and scaler.\n",
        "# Purpose:\n",
        "# Store the Logistic Regression model and the fitted scaler for use in the Streamlit app.\n",
        "\n",
        "# Import joblib for saving Python objects.\n",
        "import joblib\n",
        "\n",
        "# Save the trained model to a file.\n",
        "joblib.dump(model, \"breast_cancer_model.pkl\")\n",
        "\n",
        "# Save the fitted scaler to a file.\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "# Print confirmation.\n",
        "print(\"Model and scaler saved successfully!\")\n",
        "print(\"Model file: breast_cancer_model.pkl\")\n",
        "print(\"Scaler file: scaler.pkl\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QmhDFI79y-0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8 ‚Äì Build Streamlit App for Prediction\n",
        "# Purpose:\n",
        "# Create an interactive app where users can enter input features and get a prediction of cancer type.\n",
        "# Write everything into a file named app.py\n",
        "%%writefile app.py\n",
        "\n",
        "# Import Streamlit for building the web app UI\n",
        "import streamlit as st\n",
        "\n",
        "# Import numpy for numeric array handling\n",
        "import numpy as np\n",
        "\n",
        "# Import pickle for loading model and scaler files\n",
        "import pickle\n",
        "\n",
        "# Import dataset loader to access the breast cancer dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Import StandardScaler for feature normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Import LogisticRegression for classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# TRAIN MODEL (only runs first time)\n",
        "# --------------------------\n",
        "\n",
        "# Load Breast Cancer dataset (features + target labels)\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Split data into X (features) and y (target labels)\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Create and fit StandardScaler to normalize data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_scaled, y)\n",
        "\n",
        "# Save trained model\n",
        "with open(\"breast_cancer_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Save fitted scaler\n",
        "with open(\"scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "#  LOAD MODEL AND SCALER\n",
        "# --------------------------\n",
        "\n",
        "# Load saved model\n",
        "with open(\"breast_cancer_model.pkl\", \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Load saved scaler\n",
        "with open(\"scaler.pkl\", \"rb\") as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Streamlit UI\n",
        "# --------------------------\n",
        "\n",
        "# App title\n",
        "st.title(\" Breast Cancer Prediction App\")\n",
        "\n",
        "# Short intro text\n",
        "st.write(\"This app predicts whether a breast tumor is **Benign (1)** or **Malignant (0)** based on cell measurements.\")\n",
        "st.write(\"Enter your values below you‚Äôll also see which range is considered **safe** or **risky** based on medical data patterns.\")\n",
        "\n",
        "# Add a section header for inputs\n",
        "st.markdown(\"###  Enter The Values:\")\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Helper function to check safe ranges\n",
        "# --------------------------\n",
        "\n",
        "# Function that shows whether a given feature is within or outside its safe range\n",
        "def check_range(value, safe_min, safe_max, feature_name):\n",
        "    # If below or above safe range, show risky message\n",
        "    if value < safe_min or value > safe_max:\n",
        "        st.markdown(f\" **{feature_name}** is outside safe range ({safe_min}‚Äì{safe_max})üî¥ *Risky*\")\n",
        "    # Otherwise mark as safe\n",
        "    else:\n",
        "        st.markdown(f\" **{feature_name}** is within safe range ({safe_min}‚Äì{safe_max})üü¢ *Safe*\")\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# User Inputs (10 key features)\n",
        "# --------------------------\n",
        "\n",
        "# Numeric inputs for key cancer measurement features\n",
        "mean_radius = st.number_input(\"Mean Radius\", 5.0, 30.0, 14.0)\n",
        "check_range(mean_radius, 10, 15, \"Mean Radius\")\n",
        "\n",
        "mean_texture = st.number_input(\"Mean Texture\", 5.0, 40.0, 20.0)\n",
        "check_range(mean_texture, 10, 22, \"Mean Texture\")\n",
        "\n",
        "mean_perimeter = st.number_input(\"Mean Perimeter\", 40.0, 200.0, 90.0)\n",
        "check_range(mean_perimeter, 60, 110, \"Mean Perimeter\")\n",
        "\n",
        "mean_area = st.number_input(\"Mean Area\", 100.0, 2500.0, 600.0)\n",
        "check_range(mean_area, 400, 800, \"Mean Area\")\n",
        "\n",
        "mean_smoothness = st.number_input(\"Mean Smoothness\", 0.05, 0.2, 0.10)\n",
        "check_range(mean_smoothness, 0.07, 0.10, \"Mean Smoothness\")\n",
        "\n",
        "mean_compactness = st.number_input(\"Mean Compactness\", 0.0, 1.0, 0.15)\n",
        "check_range(mean_compactness, 0.05, 0.18, \"Mean Compactness\")\n",
        "\n",
        "mean_concavity = st.number_input(\"Mean Concavity\", 0.0, 1.0, 0.20)\n",
        "check_range(mean_concavity, 0.00, 0.18, \"Mean Concavity\")\n",
        "\n",
        "mean_concave_points = st.number_input(\"Mean Concave Points\", 0.0, 0.5, 0.10)\n",
        "check_range(mean_concave_points, 0.00, 0.10, \"Mean Concave Points\")\n",
        "\n",
        "mean_symmetry = st.number_input(\"Mean Symmetry\", 0.1, 0.4, 0.18)\n",
        "check_range(mean_symmetry, 0.14, 0.22, \"Mean Symmetry\")\n",
        "\n",
        "mean_fractal_dimension = st.number_input(\"Mean Fractal Dimension\", 0.02, 0.1, 0.06)\n",
        "check_range(mean_fractal_dimension, 0.05, 0.07, \"Mean Fractal Dimension\")\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Prepare full 30-feature input\n",
        "# --------------------------\n",
        "\n",
        "# Initialize an empty feature array (1 sample √ó 30 features)\n",
        "input_data = np.zeros((1, 30))\n",
        "\n",
        "# Store 10 entered features in the first 10 columns\n",
        "user_features = [\n",
        "    mean_radius, mean_texture, mean_perimeter, mean_area, mean_smoothness,\n",
        "    mean_compactness, mean_concavity, mean_concave_points, mean_symmetry, mean_fractal_dimension\n",
        "]\n",
        "\n",
        "# Assign user-entered values to the input vector\n",
        "input_data[0, :10] = user_features\n",
        "\n",
        "# Fill remaining 20 features with dataset mean values\n",
        "input_data[0, 10:] = np.mean(data.data[:, 10:], axis=0)\n",
        "\n",
        "# Scale input using the saved StandardScaler\n",
        "scaled_data = scaler.transform(input_data)\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Predict Button and Output\n",
        "# --------------------------\n",
        "\n",
        "# When Predict button is clicked\n",
        "if st.button(\"üîç Predict\"):\n",
        "    # Get prediction probabilities for both classes (Benign, Malignant)\n",
        "    prob = model.predict_proba(scaled_data)[0]\n",
        "    # Predict the most likely class (0 or 1)\n",
        "    prediction = model.predict(scaled_data)[0]\n",
        "    # Get maximum probability as confidence score\n",
        "    confidence = round(float(max(prob)) * 100, 2)\n",
        "\n",
        "    # Show result: if class = 1 ‚Üí Benign\n",
        "    if prediction == 1:\n",
        "        st.success(f\"üü© Prediction: **Benign (Non-cancerous)** ‚Äî Confidence: {confidence}%\")\n",
        "        st.write(\"\"\"\n",
        "        ### Meaning:\n",
        "        - Tumor cells look **normal and non-invasive**.\n",
        "        - Usually **not life-threatening**, but follow-up tests are a good practice.\n",
        "        - The cell size and texture fall mostly within safe biological ranges.\n",
        "        \"\"\")\n",
        "    # Else class = 0 ‚Üí Malignant\n",
        "    else:\n",
        "        st.error(f\"üü• Prediction: **Malignant (Cancerous)** ‚Äî Confidence: {confidence}%\")\n",
        "        st.write(\"\"\"\n",
        "        ### Meaning:\n",
        "        - Tumor cells appear **abnormal, larger, and irregular**.\n",
        "        - Indicates **possible cancerous growth**.\n",
        "        - Please consult a doctor for **biopsy and further medical evaluation**.\n",
        "        \"\"\")\n",
        "\n",
        "    # --------------------------\n",
        "    # Confidence-level interpretation\n",
        "    # --------------------------\n",
        "\n",
        "    # Add subheader for interpretation\n",
        "    st.subheader(\"Confidence Level Analysis:\")\n",
        "\n",
        "    # Create a color-coded progress bar representing confidence\n",
        "    st.progress(int(confidence))\n",
        "\n",
        "    # Categorize and explain model confidence\n",
        "    if confidence < 60:\n",
        "        st.info(\"üü¢ **Low Risk:** Model is not strongly confident of malignancy. Likely benign or safe zone.\")\n",
        "    elif 60 <= confidence < 80:\n",
        "        st.warning(\"üü† **Medium Risk:** Some patterns slightly match malignant cases ‚Äî borderline, uncertain.\")\n",
        "    else:\n",
        "        st.error(\"üî¥ **High Risk:** Strong malignant pattern detected. Please seek medical consultation immediately.\")\n",
        "\n",
        "    # Divider for readability\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # --------------------------\n",
        "    #  Educational section\n",
        "    # --------------------------\n",
        "    st.markdown(\"\"\"\n",
        "    ### How this AI works:\n",
        "    - Dataset: **Breast Cancer Wisconsin Dataset** (569 samples, 30 features)\n",
        "    - Algorithm: **Logistic Regression**\n",
        "    - The model analyzes statistical measurements like cell size, texture, compactness, etc.\n",
        "    - Output: probability (0‚Äì100%) of being **malignant or benign**.\n",
        "    \"\"\")\n",
        "\n",
        "# Add a horizontal line and disclaimer\n",
        "st.write(\"---\")\n",
        "st.caption(\"‚ö†Ô∏è Note: This tool is for **educational purposes only** and not a substitute for medical diagnosis.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bkpNh0sizCFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell purpose:\n",
        "# Start a Streamlit app and open it publicly with ngrok.\n",
        "# This version removes deprecated 'options' fields and always generates a random public URL.\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import time, os\n",
        "\n",
        "# Your ngrok authtoken\n",
        "NGROK_AUTHTOKEN = \"3536tjxZzkcNAbMjjVIegi0rOA9_4QgBc7uxDJWnwaxFDtsnq\"\n",
        "\n",
        "# Authenticate ngrok session\n",
        "ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "\n",
        "# Stop any existing tunnels and Streamlit processes\n",
        "ngrok.kill()\n",
        "os.system(\"pkill streamlit >/dev/null 2>&1\")\n",
        "os.system(\"fuser -k 8501/tcp >/dev/null 2>&1\")\n",
        "\n",
        "# Start Streamlit on port 8501\n",
        "os.system(\"streamlit run app.py --server.port 8501 &\")\n",
        "\n",
        "# Give Streamlit time to boot\n",
        "time.sleep(8)\n",
        "\n",
        "# Create a new public tunnel to port 8501 (random URL each run)\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "#  Show the link\n",
        "print(\"‚úÖ Your Streamlit app is live at:\")\n",
        "print(public_url.public_url)\n",
        "\n"
      ],
      "metadata": {
        "id": "Guk2L1_xzEtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature             | Description                                             | Higher Value Means                         |\n",
        "| ------------------- | ------------------------------------------------------- | ------------------------------------------ |\n",
        "| **Mean Radius**     | Average size of the cell nuclei in the tumor            | Larger radius ‚Üí possibly cancerous         |\n",
        "| **Mean Texture**    | Variation in the gray-scale texture (cell irregularity) | Higher ‚Üí more irregular cells              |\n",
        "| **Mean Perimeter**  | Total distance around the tumor cell cluster            | Higher ‚Üí cells are bigger/less uniform     |\n",
        "| **Mean Area**       | Total cell area                                         | Larger ‚Üí more abnormal cell growth         |\n",
        "| **Mean Smoothness** | Variation in radius lengths (surface irregularity)      | Higher ‚Üí less smooth, more irregular cells |\n"
      ],
      "metadata": {
        "id": "j50xpYbT4e6V"
      }
    }
  ]
}